{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "342aefed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow-docs\n",
      "  Downloading tensorflow_docs-2024.2.5.73858-py3-none-any.whl (182 kB)\n",
      "     -------------------------------------- 182.5/182.5 kB 1.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: nbformat in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-docs) (5.7.0)\n",
      "Requirement already satisfied: protobuf>=3.12 in c:\\users\\ucse21014\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-docs) (4.25.4)\n",
      "Collecting astor\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: absl-py in c:\\users\\ucse21014\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-docs) (2.1.0)\n",
      "Requirement already satisfied: pyyaml in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-docs) (6.0)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-docs) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->tensorflow-docs) (2.1.1)\n",
      "Requirement already satisfied: fastjsonschema in c:\\programdata\\anaconda3\\lib\\site-packages (from nbformat->tensorflow-docs) (2.16.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from nbformat->tensorflow-docs) (4.17.3)\n",
      "Requirement already satisfied: jupyter-core in c:\\programdata\\anaconda3\\lib\\site-packages (from nbformat->tensorflow-docs) (5.2.0)\n",
      "Requirement already satisfied: traitlets>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from nbformat->tensorflow-docs) (5.7.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat->tensorflow-docs) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat->tensorflow-docs) (22.1.0)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter-core->nbformat->tensorflow-docs) (305.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter-core->nbformat->tensorflow-docs) (2.5.2)\n",
      "Installing collected packages: astor, tensorflow-docs\n",
      "Successfully installed astor-0.8.1 tensorflow-docs-2024.2.5.73858\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7530b6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "from keras import layers\n",
    "from keras import ops\n",
    "from tensorflow_docs.vis import embed\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6edb9b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_channels = 3\n",
    "num_classes = 240\n",
    "image_size = 64\n",
    "latent_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df1e5c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure TensorFlow to use GPU for faster computation\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bc58603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.preprocessing.image import img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa167e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_4088\\3101677453.py:3: DtypeWarning: Columns (0,13,27,35,36,38,40,41,42,65,69,72,73,74,75,76,79,80,81,82,83,89,90,91,92,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_path = 'Segregated_Fashion_Dataset.csv'\n",
    "data = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffed0689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 30)\n"
     ]
    }
   ],
   "source": [
    "# Select necessary columns (including 'img' for image URLs)\n",
    "required_columns = ['brand','price', 'img','colour', 'Add-Ons', 'Body or Garment Size', 'Bottom Closure', 'Bottom Fabric', 'Bottom Pattern' , 'Bottom Type', 'Dupatta','Dupatta Border', 'Dupatta Fabric', 'Dupatta Pattern', 'Main Trend', 'Neck', 'Occasion', 'Pattern Coverage', 'Print or Pattern Type', 'Sleeve Length' , 'Sleeve Styling' , 'Stitch' , 'Sustainable', 'Top Design Styling' , 'Top Fabric' , 'Top Hemline' , 'Top Length' , 'Top Pattern' , 'Top Shape', 'Top Type' ]  # Adjust columns as per your dataset\n",
    "data = data[required_columns]\n",
    "\n",
    "# Take only the top 1000 rows\n",
    "data = data.head(5000)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f32eeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for storing images\n",
    "image_dir = 'fashion_images'\n",
    "if not os.path.exists(image_dir):\n",
    "    os.makedirs(image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3935835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row 447 due to missing image URL\n",
      "Skipping row 1447 due to missing image URL\n",
      "Skipping row 1456 due to missing image URL\n",
      "Skipping row 2378 due to missing image URL\n",
      "Skipping row 2680 due to missing image URL\n"
     ]
    }
   ],
   "source": [
    "# Download and preprocess images, store them in a list\n",
    "image_data = []\n",
    "valid_indices = []\n",
    "\n",
    "for i, url in enumerate(data['img']):\n",
    "    if pd.notna(url):  # Check if URL is not NaN\n",
    "        img_array = download_and_preprocess_image(url)\n",
    "        if img_array is not None:\n",
    "            image_data.append(img_array)\n",
    "            valid_indices.append(i)  # Keep track of valid images\n",
    "    else:\n",
    "        print(f\"Skipping row {i} due to missing image URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "645e48d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download and preprocess image\n",
    "def download_and_preprocess_image(url, target_size=(64, 64)):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            img = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "            img = img.resize(target_size)\n",
    "            img_array = img_to_array(img)\n",
    "            img_array = img_array.astype(\"float32\") / 255.0 #normalizing each value of pixel within the range 0 to 1\n",
    "            return img_array\n",
    "        else:\n",
    "            print(f\"Failed to download image from {url}: HTTP status code {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download image from {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a8d6f0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valid_indices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Filter data to only include rows with valid images\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39miloc[\u001b[43mvalid_indices\u001b[49m]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m image_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(image_data)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# One-hot encode categorical columns\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'valid_indices' is not defined"
     ]
    }
   ],
   "source": [
    "# Filter data to only include rows with valid images\n",
    "\n",
    "\n",
    "data = data.iloc[valid_indices].reset_index(drop=True)\n",
    "image_data = np.array(image_data)\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "categorical_columns = ['colour', 'Bottom Closure', 'Bottom Fabric', 'Bottom Pattern' , 'Bottom Type', 'Neck', 'Pattern Coverage', 'Print or Pattern Type', 'Sleeve Length' , 'Sleeve Styling' , 'Top Design Styling' , 'Top Fabric' , 'Top Hemline' , 'Top Length' , 'Top Pattern' , 'Top Shape', 'Top Type' ]  # Adjust columns as per your dataset\n",
    "\n",
    "#Converting into a Pandas dataframe\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Handling missing values for all categorical columns\n",
    "for col in categorical_columns:\n",
    "    data[col].fillna('Unknown', inplace=True)  # Replace missing values with 'Unknown'\n",
    "# Removing rows with missing values\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "#Initialize OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Apply one-hot encoding to the categorical columns\n",
    "one_hot_encoded = encoder.fit_transform(df[categorical_columns])\n",
    "\n",
    "\n",
    "#Create a DataFrame with the one-hot encoded columns\n",
    "#We use get_feature_names_out() to get the column names for the encoded data\n",
    "one_hot_df = pd.DataFrame(one_hot_encoded, columns=encoder.get_feature_names_out(categorical_columns))\n",
    "\n",
    "# Concatenate the one-hot encoded dataframe with the original dataframe\n",
    "df_encoded = pd.concat([df, one_hot_df], axis=1)\n",
    "\n",
    "# Drop the original categorical columns\n",
    "df_encoded = df_encoded.drop(categorical_columns, axis=1)\n",
    "\n",
    "df_encoded = df_encoded.drop('img', axis=1)\n",
    "df_encoded = df_encoded.drop('brand', axis=1)\n",
    "df_encoded = df_encoded.drop('Add-Ons', axis=1)\n",
    "df_encoded = df_encoded.drop('Body or Garment Size', axis=1)\n",
    "df_encoded = df_encoded.drop('Occasion', axis=1)\n",
    "df_encoded = df_encoded.drop('Stitch', axis=1)\n",
    "df_encoded = df_encoded.drop('Sustainable', axis=1)\n",
    "df_encoded = df_encoded.drop('Dupatta', axis=1)\n",
    "df_encoded = df_encoded.drop('Dupatta Border', axis=1)\n",
    "df_encoded = df_encoded.drop('Dupatta Fabric', axis=1)\n",
    "df_encoded = df_encoded.drop('Dupatta Pattern', axis=1)\n",
    "df_encoded = df_encoded.drop('Main Trend', axis=1)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Handle missing values in the price column by filling with the median price\n",
    "df_encoded['price'].fillna(df_encoded['price'].median(), inplace=True)\n",
    "\n",
    "# Initialize the StandardScaler to normalize the price\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Reshape the price column to fit the scaler and scale it\n",
    "df_encoded['price'] = scaler.fit_transform(df_encoded[['price']])\n",
    "\n",
    "# Display the first few rows of the processed dataframe to confirm the changes\n",
    "print(f\"Processed data with scaled price: \\n{df_encoded.head()}\")\n",
    "\n",
    "\n",
    "# Display the resulting dataframe\n",
    "print(f\"Encoded data : \\n{df_encoded}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e740e6d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_encoded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatures shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mdf_encoded\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabels shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, image_data\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_encoded' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Features shape:\", df_encoded.shape)\n",
    "print(\"Labels shape:\", image_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75efdee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (3496, 64, 64, 3)\n",
      "Shape of y_train: (3496, 240)\n",
      "Shape of X_test: (1499, 64, 64, 3)\n",
      "Shape of y_test: (1499, 240)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(image_data, df_encoded, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(buffer_size=1024).batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size)\n",
    "\n",
    "# Example of the shape\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}\")\n",
    "print(f\"Shape of y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "665ae89e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m all_images \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([\u001b[43mX_train\u001b[49m, X_test])\n\u001b[0;32m      2\u001b[0m all_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([y_train, y_test])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "all_images = np.concatenate([X_train, X_test])\n",
    "all_labels = np.concatenate([y_train, y_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19860ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training images: (4995, 64, 64, 3)\n",
      "Shape of training labels: (4995, 240)\n"
     ]
    }
   ],
   "source": [
    "# Create tf.data.Dataset.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((all_images, all_labels))\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "print(f\"Shape of training images: {all_images.shape}\")\n",
    "print(f\"Shape of training labels: {all_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd8bdef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368 243\n"
     ]
    }
   ],
   "source": [
    "generator_in_channels = latent_dim + num_classes\n",
    "discriminator_in_channels = num_channels + num_classes\n",
    "print(generator_in_channels, discriminator_in_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a8afb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the discriminator.\n",
    "discriminator = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.InputLayer((64, 64, discriminator_in_channels)),  # Input shape is 64x64x3\n",
    "        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(negative_slope=0.2),\n",
    "        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(negative_slope=0.2),\n",
    "        layers.Conv2D(256, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(negative_slope=0.2),\n",
    "        layers.Conv2D(256, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(negative_slope=0.2),\n",
    "        layers.GlobalMaxPooling2D(),\n",
    "        layers.Dense(1),  # Binary classification: Real (1) or Fake (0)\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n",
    "\n",
    "# Create the generator.\n",
    "generator = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.InputLayer((128 + 240,)),  # Latent dimension + number of classes\n",
    "        layers.Dense(8 * 8 * 256),\n",
    "        layers.LeakyReLU(negative_slope=0.2),\n",
    "        layers.Reshape((8, 8, 256)),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(negative_slope=0.2),\n",
    "        layers.Conv2DTranspose(64, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(negative_slope=0.2),\n",
    "        layers.Conv2DTranspose(3, (4, 4), strides=(2, 2), padding=\"same\", activation=\"tanh\"),  # RGB output\n",
    "    ],\n",
    "    name=\"generator\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7410980",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalGAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super().__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.seed_generator = keras.random.SeedGenerator(1337)\n",
    "        self.gen_loss_tracker = keras.metrics.Mean(name=\"generator_loss\")\n",
    "        self.disc_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.gen_loss_tracker, self.disc_loss_tracker]\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super().compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data.\n",
    "        real_images, one_hot_labels = data\n",
    "\n",
    "        # Add dummy dimensions to the labels so that they can be concatenated with\n",
    "        # the images. This is for the discriminator.\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "        image_one_hot_labels = ops.repeat(\n",
    "            image_one_hot_labels, repeats=[image_size * image_size]\n",
    "        )\n",
    "        image_one_hot_labels = ops.reshape(\n",
    "            image_one_hot_labels, (-1, image_size, image_size, num_classes)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space and concatenate the labels.\n",
    "        # This is for the generator.\n",
    "        batch_size = ops.shape(real_images)[0]\n",
    "        random_latent_vectors = keras.random.normal(\n",
    "            shape=(batch_size, self.latent_dim), seed=self.seed_generator\n",
    "        )\n",
    "        random_vector_labels = ops.concatenate(\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\n",
    "        )\n",
    "\n",
    "        # Decode the noise (guided by labels) to fake images.\n",
    "        generated_images = self.generator(random_vector_labels)\n",
    "\n",
    "        # Combine them with real images. Note that we are concatenating the labels\n",
    "        # with these images here.\n",
    "        fake_image_and_labels = ops.concatenate(\n",
    "            [generated_images, image_one_hot_labels], -1\n",
    "        )\n",
    "        real_image_and_labels = ops.concatenate([real_images, image_one_hot_labels], -1)\n",
    "        combined_images = ops.concatenate(\n",
    "            [fake_image_and_labels, real_image_and_labels], axis=0\n",
    "        )\n",
    "\n",
    "        # Assemble labels discriminating real from fake images.\n",
    "        labels = ops.concatenate(\n",
    "            [ops.ones((batch_size, 1)), ops.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "\n",
    "        # Train the discriminator.\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space.\n",
    "        random_latent_vectors = keras.random.normal(\n",
    "            shape=(batch_size, self.latent_dim), seed=self.seed_generator\n",
    "        )\n",
    "        random_vector_labels = ops.concatenate(\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\n",
    "        )\n",
    "\n",
    "        # Assemble labels that say \"all real images\".\n",
    "        misleading_labels = ops.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_images = self.generator(random_vector_labels)\n",
    "            fake_image_and_labels = ops.concatenate(\n",
    "                [fake_images, image_one_hot_labels], -1\n",
    "            )\n",
    "            predictions = self.discriminator(fake_image_and_labels)\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        # Monitor loss.\n",
    "        self.gen_loss_tracker.update_state(g_loss)\n",
    "        self.disc_loss_tracker.update_state(d_loss)\n",
    "        return {\n",
    "            \"g_loss\": self.gen_loss_tracker.result(),\n",
    "            \"d_loss\": self.disc_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d43381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 2s/step - d_loss: 0.3663 - g_loss: 3.6035\n",
      "Epoch 2/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 2s/step - d_loss: 0.0915 - g_loss: 7.6142\n",
      "Epoch 3/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 2s/step - d_loss: 0.0630 - g_loss: 7.9836\n",
      "Epoch 4/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 2s/step - d_loss: 0.0202 - g_loss: 9.1472\n",
      "Epoch 5/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 2s/step - d_loss: 0.2964 - g_loss: 5.4650\n",
      "Epoch 6/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 2s/step - d_loss: 0.0481 - g_loss: 4.3103\n",
      "Epoch 7/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 2s/step - d_loss: 0.0224 - g_loss: 5.0526\n",
      "Epoch 8/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 2s/step - d_loss: 0.2199 - g_loss: 9.1824\n",
      "Epoch 9/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 2s/step - d_loss: 0.0115 - g_loss: 7.9849\n",
      "Epoch 10/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 2s/step - d_loss: 0.0038 - g_loss: 8.1500\n",
      "Epoch 11/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 2s/step - d_loss: 0.0011 - g_loss: 8.9593\n",
      "Epoch 12/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2s/step - d_loss: 9.8738e-04 - g_loss: 9.3489\n",
      "Epoch 13/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 2s/step - d_loss: 0.0029 - g_loss: 8.6221\n",
      "Epoch 14/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 2s/step - d_loss: 0.1424 - g_loss: 9.1621\n",
      "Epoch 15/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 2s/step - d_loss: 0.1555 - g_loss: 3.1630\n",
      "Epoch 16/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 2s/step - d_loss: 0.1789 - g_loss: 3.7052\n",
      "Epoch 17/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 2s/step - d_loss: 0.3491 - g_loss: 3.1989\n",
      "Epoch 18/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 2s/step - d_loss: 0.4941 - g_loss: 1.9379\n",
      "Epoch 19/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2s/step - d_loss: 0.5297 - g_loss: 1.5551\n",
      "Epoch 20/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2s/step - d_loss: 0.6772 - g_loss: 1.2677\n",
      "Epoch 21/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - d_loss: 0.5216 - g_loss: 1.3214\n",
      "Epoch 22/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 2s/step - d_loss: 0.5310 - g_loss: 1.4003\n",
      "Epoch 23/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2s/step - d_loss: 0.3403 - g_loss: 2.1045\n",
      "Epoch 24/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 2s/step - d_loss: 0.7120 - g_loss: 0.8313\n",
      "Epoch 25/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 2s/step - d_loss: 0.6628 - g_loss: 0.9394\n",
      "Epoch 26/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - d_loss: 0.5816 - g_loss: 1.2962\n",
      "Epoch 27/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - d_loss: 0.5269 - g_loss: 1.3783\n",
      "Epoch 28/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 2s/step - d_loss: 0.3910 - g_loss: 2.0063\n",
      "Epoch 29/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 2s/step - d_loss: 0.4838 - g_loss: 1.9364\n",
      "Epoch 30/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 2s/step - d_loss: 0.6610 - g_loss: 1.4887\n",
      "Epoch 31/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 2s/step - d_loss: 0.4611 - g_loss: 1.3483\n",
      "Epoch 32/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 2s/step - d_loss: 0.6447 - g_loss: 1.0922\n",
      "Epoch 33/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 2s/step - d_loss: 0.4005 - g_loss: 1.6355\n",
      "Epoch 34/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 2s/step - d_loss: 0.6029 - g_loss: 1.4753\n",
      "Epoch 35/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2s/step - d_loss: 0.5612 - g_loss: 1.8569\n",
      "Epoch 36/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 2s/step - d_loss: 0.4774 - g_loss: 1.8721\n",
      "Epoch 37/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 2s/step - d_loss: 0.4250 - g_loss: 1.6901\n",
      "Epoch 38/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 2s/step - d_loss: 0.4439 - g_loss: 1.6138\n",
      "Epoch 39/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 2s/step - d_loss: 0.6484 - g_loss: 1.1663\n",
      "Epoch 40/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 2s/step - d_loss: 0.5120 - g_loss: 1.6218\n",
      "Epoch 41/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 2s/step - d_loss: 0.4846 - g_loss: 2.3365\n",
      "Epoch 42/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 2s/step - d_loss: 0.4326 - g_loss: 2.2465\n",
      "Epoch 43/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 2s/step - d_loss: 0.3858 - g_loss: 1.6494\n",
      "Epoch 44/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 2s/step - d_loss: 0.6834 - g_loss: 0.9430\n",
      "Epoch 45/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 2s/step - d_loss: 0.7167 - g_loss: 1.0928\n",
      "Epoch 46/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2s/step - d_loss: 0.6296 - g_loss: 0.9802\n",
      "Epoch 47/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 2s/step - d_loss: 0.6300 - g_loss: 0.8334\n",
      "Epoch 48/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2s/step - d_loss: 0.5913 - g_loss: 0.9649\n",
      "Epoch 49/100\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 2s/step - d_loss: 0.5288 - g_loss: 1.3878\n",
      "Epoch 50/100\n",
      "\u001b[1m76/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m5s\u001b[0m 2s/step - d_loss: 0.5270 - g_loss: 1.5789"
     ]
    }
   ],
   "source": [
    "cond_gan = ConditionalGAN(\n",
    "    discriminator=discriminator, generator=generator, latent_dim=latent_dim\n",
    ")\n",
    "cond_gan.compile(\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    ")\n",
    "\n",
    "cond_gan.fit(dataset, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6ec31db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.losses.losses.BinaryCrossentropy at 0x1f72feeaf90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn=keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c36020-65a3-4763-a2bc-70af323cd92b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
